# S02 Intro to ML, the concepts

## outline

- Introduction:
  - title: Introduction to ML
- content:
  - What is the concept of teaching a computer using data: what is machine learning
  - Cycle of a data science project
  - Vocabulary: model, inference, training,
  - Applications of Machine Learning: nature of data, multimodality,
  - Stats vs Machine Learning: explanation vs inference
  - illustrate with questions: find factors influencing vs predicting; understanding vs inference; ex users click on website
  - black box: interpretability, recruitment, credit
  - Supervised vs non-supervised, ground truth data
  - Regression, Classification, and Clustering
  - Choosing the right Metric, scoring
  - The importance of Benchmarking
  - It's all about the data: [UCI ML](https://en.wikipedia.org/wiki/University_of_California,_Irvine#Machine_Learning_Repository) repo (1978), tabular data
  - Kaggle competitions: titanic, boston housing

## What is machine learning ?

Let's start by the beginning.

Good question

<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>
<img src="./../img/memes/xkcd_machine_learning_2x.png"  style='display: block; margin: auto;' alt= "Machine Learning by XKCD"> from <a href="https://xkcd.com/1838/">XKCD</a>
</div>

---

### Let's ask an AI

**my prompt**: concise definition of machine learning

**[ChatGPT 4o](https://chatgpt.com/)**:

    Machine Learning: A branch of artificial intelligence (AI) where computers learn patterns from data and improve performance on tasks without being explicitly programmed.

**[Gemini](https://gemini.google.com/app)**:

    Machine learning is a field of computer science that enables computers to learn from data without being explicitly programmed.

**[Claude 3.5 Sonnet](https://claude.ai/)**:

    Machine learning is the field of computer science focused on developing algorithms and statistical models that enable computer systems to perform tasks through pattern recognition and inference, rather than explicit programming. The systems improve their performance with experience/data.

**[DeepSeek](https://chat.deepseek.com)** :

    Machine learning is a subset of artificial intelligence that involves training algorithms to recognize patterns and make predictions or decisions based on data, without being explicitly programmed for each task.

see also the definition from [wikipedia](https://en.wikipedia.org/wiki/Machine_learning):

    Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1]

Let's inspect the DeepSeek definition.

- Machine learning is a subset of artificial intelligence
  - subset of AI
- training algorithms to recognize patterns
  - training a model on a dataset (true data subset)
- without being explicitly programmed
  - minimizing the error rate
  - reducing a loss function,
- make predictions or decisions
  - inference
  - predictions on new data, data that was not in the training subset

---

### Hands on definition

A more practical definition could be:

<div style='display: block; margin: auto; width : 80%;  padding: 10px; margin-bottom: 5px; border: 1px solid #66F'>
<strong>Training</strong> a <strong>model</strong> on a <strong>dataset</strong> in order to use it for <strong>predictions</strong> on previously <strong>unseen data</strong>
</div>


![Illustrate the machine learning definition](./../img/slop/definition-machine-learning.png)

## **dataset**:

tabular data = csv, xlsx, spreadsheet

but also

- text,
- images,
- sound,
- videos
- sensor data (IoT)


Some vocabulary

- **target variable**: the variable you want to predict : y
- predictors, **features** : the variable that are used to rpedict the target variable
- **feature engineering**: transforming predictors to increase performance


## Models


### regression model:

y = a x_1 + b x_2 + c

<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>
<img src="./../img/hyperplane_3d.png"  style='display: block; margin: auto;' alt= "Hyperplace regression">
</div>

### decision tree

<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>
<img src="./../img/decision-tree-classification-algorithm.png"  style='display: block; margin: auto;' alt= "Decision tree">
</div>

### neural net

<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>
<img src="./../img/feed_forward.png"  style='display: block; margin: auto;' alt= "Feed Forward Neural Network">
</div>


## **training**:

- adapting the internal parameters of the model in an _automated_ way in order to improve some internal scoring aka loss function
- loss function: the criteria that drives the automatic modifications of internal parameters of the model

## **predictions**:

aka inference

- applying the trained model on new data.

## **unseen data**:


Robust, generalization,

- If the model is well trained the predictions on new data are accurate. The model is able to generalize, it is robust


## Cycle of a data science project

It's all about the data and the business

<div style='display: block; margin: auto; width : 80%;  padding: 10px;'>
<img src="./../img/data_science_workflow.png"  style='display: block; margin: auto;' alt= "Cycle of a data science project">
</div>


Several important elements here

### Data

- the data. It is key.

see this interview from [Andrew NG](https://www.andrewng.org/)



How do you define data-centric AI, and why do you consider it a movement?

  Ng: Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.

from https://spectrum.ieee.org/andrew-ng-data-centric-ai


#### Small data vs Big data

What's Small data ? What's Big data ?

It's a rather relative definition and Big data is a buzzword.
My very personal definition of big data is a dataset that can't be loaded in RAM.
So a few Gb. But that may be conservative.


The nature of the data also dictates the small vs big data consideration.

From small to big

- tabular: numbers, and text (categorical, paragraphs, ...)
- IoT : sensors, numerical but massive (see also stocks)
- images: b&w, RGB, ..
- audio
- videos

In this course, we'll work on tabular data which is small.
Something that can be loaded in a google spreadsheet or Excel file.

## Data quality and relevance, signal vs noise

It's not because you have data that you can train a model for inference.

<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>
<img src="./../img/spurious_correlations.png"  style='display: block; margin: auto;' alt= "spurious correlations"> from <a href="http://www.tylervigen.com/spurious-correlations">Spurious Correlations</a>
</div>


Data quality is essential.

- missing values
- outliers
- typos
- different data types
- access, ownership, privacy

## Feature engineering

- what variables are relevant ?

  - Is the hair color relevant to assess the value of a candidate ?
  - Is the acne on a teenager face a good predictor of weight ?


- how to create new variables and why ?

You can transform existing variables into new variables with the goal of making the data easier to ingest by the model.

For instance:
  - log(1+x)
  - x^2, x^(1/2), ...
  - abs(x)
  - x*y, x+y, x/(1+y), ...
  - exp(x),
  - ...

For instance: log(1+ x) is often used on a positive variable to make its distribution more normal, more gaussian (bell curve). And Gaussianity is often an important aspect of data normalization making things easier to comprehend.

<div style='display: block; margin: auto; width : 80%;  padding: 10px;'>
<img src="./../img/lognormal-distribution.jpg"  style='display: block; margin: auto;' alt= "Log normal distribution">
</div>


You can also import external data to complement your dataset.

For instance, road works data to complement traffic predictions in Paris, weather to predict the sales of umbrellas, gas prices to predict airplane fares, etc etc

- how to chose the important variables ?

### Curse of dimension

Scenario: You have 200 samples but also 200 variables ?  There's not enough data samples for the model to understand the data. This is called the **curse of dimensionality**

As the number of dimensions increases, the data becomes increasingly sparse, making it harder to find meaningful patterns or relationships.

<div style='display: block; margin: auto; width : 80%;  padding: 10px;'>
<img src="./../img/curse_dimension.png"  style='display: block; margin: auto;' alt= "curse of dimensionality">
</div>

Example Genomics:

**Problem**: Genomic data often has thousands of features (e.g., gene expressions) but relatively few samples.

**Challenge**: The sparsity of data makes it difficult to identify patterns or biomarkers for diseases.

**Solution**: Feature selection or dimensionality reduction methods like t-SNE or UMAP are used to focus on relevant genes.

## Business relevance and performance

Given the correct data , it's always possible to train some model for inference. To make sure the model is efficient you choose a metric, a scoring.

- classification: accuracy, recall, precision, ...
- regression: RMSE, MAE, ...
- clustering: silhouette score, ...

But the real difficulty lies in making sure that the chosen metric truly reflects the proper value for the business.

selecting the right performance metric is crucial because the metric directly influences how the model is optimized and evaluated. If the chosen metric does not align with the business value, it can lead to suboptimal decisions, misleading insights, and even financial losses.

### Classification
For instance, what's the costs of False negatives vs False positives?

- in cyber security intrusion detection. It's better to lean towards false positives (intrusion detected) than to miss one
- health, cancer detection: same, False negatives end up costing more.
- call center, calling the people most likely to subscribe. Is it be better to miss calling a good prospect or waste time calling people who are not likely to buy your product ?

Medical Diagnosis: Prioritizing Accuracy Over Recall

**Scenario**: A model is trained to classify patients as having a disease or not. The model achieves high accuracy by predicting "no disease" for most cases, as the disease is rare.

**Business Impact**: False negatives (missed diagnoses) are extremely costly, as they delay treatment and can lead to severe health complications or death. False positives, while undesirable, are less harmful as they can be corrected with further testing.

**Better Metric**: Recall (Sensitivity) should be prioritized to minimize false negatives. Alternatively, a weighted cost function that penalizes false negatives more heavily than false positives can be used.

### Regression

Demand Forecasting

**Scenario**: A retail company uses MSE to evaluate a demand forecasting model for perishable goods. The model minimizes average prediction errors but consistently overestimates demand for high-value items.

**Business Impact**: Overestimation leads to overstocking, resulting in waste and increased costs for perishable items. Underestimation, on the other hand, leads to stock depletion and lost sales.

**Better Metric**: A weighted error metric that penalizes overestimation more heavily than underestimation, or a custom loss function that aligns with the cost of overstocking versus stockouts, would better reflect business priorities

Bike Rental Demand Prediction:

**Scenario**: A bike-sharing company uses RMSE to evaluate a model predicting daily bike rentals. The model has low RMSE but consistently underestimates demand during peak hours.

**Business Impact**: Underestimation during peak hours leads to insufficient bike availability, frustrating customers and reducing revenue.

**Better Metric**: Analyzing error bias (e.g., overestimation vs. underestimation) and using a metric that penalizes underestimation more heavily during peak hours would align better with business objectives



## Stats vs Machine Learning: explanation vs inference

There are 2 main schools of thoughts, 2 disciplines when it comes to modelization: statistics and machine learning


Consider a dataset of users on a website. The dataset includes many variables

- behavior on the web site and on
- behavior on some other websites
- profiles and demographics

You are analyzing the conversion rate. Whether the user will buy or subscribe to something you have just launched, a newsletter, a new product etc

Questions you might have

- what are the factors that influence conversion? could it be the age of the person? the amount of time they have been registered users? whether they already bought similar products ? etc ...
- here's a  user and related data, what is the probability he or she will buy the new product

The 1st question is a statistical modeling question. The goal is to find the dynamics of variables that best *explain* the data.

The 2nd questions focuses on being able to make **predictions** on new samples. This is where  machine learning shines.
We don't care so much about the why. Just that the predictions are efficient, accurate, cost relevant.

Leo Breiman two cultures paper: Statistical Science 2001, Vol. 16, No. 3, 199–231 Statistical Modeling: [The Two Cultures](./../breiman-two-cultures-1009213726.pdf)  : Either explain Nature or predict and don't care


<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>
<img src="./../img/predictable-Interpretable.png"  style='display: block; margin: auto;' alt= "balance between predictability and interpretability">
</div>

There's a balance between **predictability** and **interpretability**

- Models that are good at prediction are often more complex
- Models that are easy to interpret are simple, and therefore, worse predictors

For instance: Decision Trees are super intuitive, but can’t model complex processes while Random Forests have excellent prediction accuracy, but are basically impossible to interpret.

### Interpretability

Some context require machine learning predictions to be explainable.

For instance credit applications. The bank must be able to show why a certain application was rejected or approved

In HR, where discrimination is to be avoided, decision must also be explainable. You have to be able to justify the decision of hiring or not hiring a candidate. You can't just say: The model made the decision.


Model that cannot be explained are called **black boxes**.

- linear regression is not a black box model
- neural networks are black box models
- tree-based models are black box models when multiple trees are ensembled.

Techniques exist to interpret black box models: [shap](https://christophm.github.io/interpretable-ml-book/shap.html), [lime](https://christophm.github.io/interpretable-ml-book/lime.html)

I strongly recommend that online book on [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/): A Guide for Making Black Box Models Explainable by [Christoph Molnar](https://bsky.app/profile/christophmolnar.bsky.social)


## Supervised vs non-supervised

Supervised learning and unsupervised learning are two primary types of machine learning, each with distinct characteristics and applications.

Supervised learning uses labelled data for tasks like classification, while unsupervised learning identifies patterns in unlabelled data.

There's a subset of the data that is already labeled. For a classification task we know what categories each sample belongs to. We use that labelled data to train the model.

<div style='display: block; margin: auto; width : 80%;  padding: 10px;'>
<img src="./../img/ch1_supervised_learning.png"  style='display: block; margin: auto;' alt= "Supervised learning">
</div>

Supervised learning is used for classification and regression.

Unsupervised: We do not know the labels of the data, we want to find patterns in that data.

<div style='display: block; margin: auto; width : 80%;  padding: 10px;'>
<img src="./../img/ch1_unsupervised_learning.png"  style='display: block; margin: auto;' alt= "Unsupervised learning">
</div>

Unsupervised learning is used for clustering, dimension reduction and crystal ball divination

### An example

Look at the `weight_height.csv` dataset

https://github.com/SkatAI/efrei-ml/blob/master/data/weight_height.csv


**Supervised learning**: We can predict the weight of children based on age, height and sex


<div style='display: block; margin: auto; width : 80%;  padding: 10px; margin-bottom: 5px; border: 1px solid #66F'>
weight ~ age + height + sex
</div>

**Unsupervised learning**: group the children that are similar

<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>
<img src="./../img/clustering_example.png"  style='display: block; margin: auto;' alt= "Clustering example">
</div>


## Classification vs Regression

In supervised learning, there are 2 main tasks : **classification** and **regression**.

Regression concerns continuous target variables : predicting the price of a house, the number of visitors to a website, downloads of an app , or the temperature, ... any variable that has real, continuous, values

Classification concerns predicting categories. yes vs no. true vs false, Male vs Female, dog vs cat, up vs down etc

Classification can be further split into:

- binary classification. only 2 categories are possible and mutually exclusive
- multi class classification. N >2 categories are possible and mutually exclusive. think of colors, types of cars, species of animals or trees, etc
- Multilabel Classification: N > 2 categories which are not mutually exclusive. For instance genre of films, documents, etc ...


## Clustering

Clustering is an Unsupervised learning tasks, where similar samples are grouped together. It is used to understand patterns in the data that are not explicit. For instance, clustering shoppers to understand behavior, or something similar.

## Dimension reduction with PCA

Imagine a dataset with 2 variables. easy to represent visually on a plot.
Something like a scatter plot will suffice.

<div style='display: block; margin: auto; width : 80%;  padding: 10px;'>
<img src="./../img/scatterplot-2d.png"  style='display: block; margin: auto;' alt= "Scatter plot 2d">
</div>



if we have 3 dimensions, we can do a 3D scatterplot.

<div style='display: block; margin: auto; width : 80%;  padding: 10px;'>
<img src="./../img/scatterplot-3d.png"  style='display: block; margin: auto;' alt= "Scatter plot 3d">
</div>


But after that, visualizing the data gets tricky.

So we need dimension reduction. An unsupervised task that uses PCA.

Input a dataset with N variables,
output a dataset with 2 variables
plot the 2 variables

Dimension reduction is also useful when you have a dataset with a lot of dimensions., a lot of variables, but not enough samples.

Then you reduce the dimensions of the dataset so that there's enough signal for the model to train on.

Dimension reduction with PCA is a standard pre processing step in machine learning.

## Choosing the right Metric

How do you evaluate the performance of a model ?

### classification

Simple case of binary classification, let's imagine a model that classifies photos of cats and dogs into their respective categories.

<div style='display: block; margin: auto; width : 80%;  padding: 10px;'>
<img src="./../img/cats_dogs.png"  style='display: block; margin: auto;' alt= "Classification Cats vs Dogs">
</div>



Or a model that predicts if you're going to get fired, already have cancer, be pregnant, win the lottery, commit a crime etc etc

What sort of metrics can you think of that would measure the performance of a classification model ?

first decide which class is the positive one and which class is the negative one.

Then think in terms of True positive, True Negatives, False positives and False negatives.

<div style='display: block; margin: auto; width : 80%;  padding: 10px;'>
<img src="./../img/confusion_matrix.png"  style='display: block; margin: auto;' alt= "Classification Cats vs Dogs confusion_matrix">
</div>

To illustrate the difference between False positives and False Negatives.

<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>
<img src="./../img/pregnant.png"  style='display: block; margin: auto;' alt= "False positives and False Negatives">
</div>


- Accuracy : TP + TN / population
- Recall:
- Precision

and many others

### Regression

You want to estimate the distance between the predicted values and the real values.

Potential metrics include

- RMSE
- MAE
- MAPE

Note that the RMSE is relative to the range of the values of the target variable.

For house prizes the predicted values are in the Millions, while for temperatures they range roughly from -100 to +100.

## Benchmark

So you train your first model, choose an evaluation metric, and you get a score.

How do you know if that's a good score or a meh one?

Let's say you want to predict credit fraud.

your model predicts fraud with an accuracy of 55%. is that good ? bad ?

Would a simpler solution give the same performance ?

So in all your data science projects you first need to establish a benchmark, a basic performance with a simple model or a rule.

For instance, a benchmark for predicting the temperature tomorrow is ... that it will be the same as today. can't get ,much simpler than that. It so happens that this is hard to beat. It's a great benchmark.

In a binary classification task, can you do better than a coin flip ?

If you don't establish a benchmark, you don't know if all the effort you put in your model and training an feature engineering is really worth it in the end.

## Data

One constant in all machine learning projects is that we need data.

data is everywhere but there are a couple of online resources to get good data

- The UCI machine learning repository <https://archive.ics.uci.edu/>
- Kaggle the machine learning competition platform <https://www.kaggle.com/>

The UCI ML repository was created in 1978! to host dataset for machine learning. It holds over 670 datasets for classification, regression, clustering, time series etc

A few classic datasets you should be familiar with are

- [Iris](https://archive.ics.uci.edu/dataset/53/iris) created in 1936

<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>
<img src="./../img/iris-machinelearning.png"  style='display: block; margin: auto;' alt= "img/iris dataset machinelearning">
</div>


- [Titanic](https://www.kaggle.com/competitions/titanic/data)

- [Boston housing](https://www.kaggle.com/datasets/schirmerchad/bostonhoustingmlnd)

and many other fun datasets and competitions.

## Recap

- supervised vs unsupervised
- classification vs regression
- metrics
